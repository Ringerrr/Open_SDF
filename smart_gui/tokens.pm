package tokens;

use strict;
use warnings;

use constant ENV_TOKENS => {
    ENV_JOB_ID              => {
        short_desc          => "Unique job ID of the current job"
      , long_desc           => "This token can be used to stamp the current job ID into target tables, to aid in data auditing."
  }
  , ENV_HARVEST_PATH        => {
        short_desc          => "Path to the current file"
      , long_desc           => "Harvest mode augments SFX with a wrapper ( batch ) process that scans a directory for files to process."
  }
  , ENV_HARVEST_FILENAME    => {
        short_desc          => "Current filename"
      , long_desc           => "The filename *only* being processed ( ie no path  )."
  }
  , ENV_TIMESTAMP           => {
        short_desc          => "Current timestamp"
      , long_desc           => "Returns the timestamp as of the exact time the parameter is being resolved."
  }
  , ENV_LOG_DIR             => {
        short_desc          => "Log directory"
      , long_desc           => "Returns the full path to the log directory."
  }
  , ENV_HIGH_DATE           => {
        short_desc          => "Returns the high date constant"
      , long_desc           => "This is generally 2999-12-31."
  }
  , ENV_RECORDS_AFFECTED    => {
        short_desc          => "# of records last affected"
      , long_desc           => "Where possible, SFX captures and remembers the number of records affected by the last operation."
  }
  , ENV_CONTROL_DB_NAME     => {
        short_desc          => "The name of the DB hosting our control metadata"
      , long_desc           => "This is where all the templates, parameters, jobs, etc are stored."
  }
  , ENV_LOG_DB_NAME     => {
        short_desc          => "The name of the DB hosting our log metadata"
      , long_desc           => "This is where all the batches, jobs, and logs are stored."
  }
  , ENV_EXTRACT_DATE        => {
        short_desc          => "The date currently being processed"
      , long_desc           => "In harvest mode when we detect an extract date, or where a date gets passed in, this token will return that date."
  }
};

use constant COMPLEX_TOKENS => {
    COMPLEX_JOIN_ON_KEYS    => {
        short_desc          => "Generate a join of 2 tables, based on a list of keys"
      , long_desc           => "This token joins tables named SOURCE and TARGET ( alias them ), on the keys specified in #P_KEYS#."
                             . " If #P_ZZ_CASE_INSENSITIVE_VERSIONING# is set, the join condition generated will be case insensitive."
    }
  , COMPLEX_SOURCE_KEYS     => {
        short_desc          => "Return a source-prefixed list of keys"
      , long_desc           => "Keys found in #P_KEYS# are returned in a string, prefixed with the SOURCE identifier"
  }
  , COMPLEX_SCD2_ATTRIBUTE_CHANGED => {
        short_desc          => "SCD2 - Generate a filter to detect records changed between SOURCE and TARGET"
      , long_desc           => "The filter generated by this token is intended for SCD2 versioning, to detect changed records ( ie to close in the target table )."
                             . "\n\nIf #P_ZZ_IGNORE_COLUMNS# is set, these columns will not be included in the filter."
                             . "\n\nIf #P_ZZ_CASE_INSENSITIVE_VERSIONING# is set, the comparision generated will be case insensitive."
  }
  , COMPLEX_SOURCE_KEY_IS_NULL => {
        short_desc          => "SCD2 - Generate a filter to detect records in the TARGET that don't exist in the SOURCE"
      , long_desc           => "The filter generated by this token is intended for SCD2 versioning, to detect records that should be CLOSED in the target"
                             . " if the record has been DELETED from the source."
  }
  , COMPLEX_TARGET_KEY_IS_NULL => {
        short_desc          => "SCD2 - Generate a filter to detect records in the SOURCE that don't exist in the TARGET"
      , long_desc           => "The filter generated by this token is intended for SCD2 versioning, to detect records that should be INSERTED into the target"
                             . " after changes records have been closed."
  }
  , COMPLEX_COLUMNS_FROM_SOURCE => {
        short_desc          => "Generate a list of columns from the source table"
      , long_desc           => "Queries the source table ( #CONFIG_SOURCE_TABLE_NAME# ) and returns a comma-separated list of columns."
                             . "\n\nColumns in #P_IGNORE_SOURCE_COLS# will be ignored."
                             . "\n\nIf #P_ZZ_SKIP_SOURCE_FORMATTING# is set, then no data mangling will occur ( to format data in a standard format suitable for DB migrations )"
  }
  , COMPLEX_SOURCE_PREFIXED_COLUMNS_FROM_TARGET => {
        short_desc          => "Generate a source-prefixed list of columns from the target table"
      , long_desc           => "Queries the target table ( #CONFIG_TARGET_TABLE_NAME# ) and returns a source-prefixed ( ie SOURCE.col_1 etc ) list of columns."
                             . "\n\nColumns in #P_ZZ_IGNORE_COLUMNS# are ignored."
  }
  , COMPLEX_COLUMNS_FROM_TARGET  => {
        short_desc          => "Generate a list of columns from the target table"
      , long_desc           => "Queries the target table ( #CONFIG_TARGET_TABLE_NAME# ) and returns a list of columns."
                             . "\n\nColumns in #P_ZZ_IGNORE_COLUMNS# are ignored."
  }
  , COMPLEX_BIGQUERY_SCD1_MERGE_COLUMNS_FROM_SOURCE_IF_EXISTS => {
        short_desc          => "SCD1 logic for BigQuery that merges in new columns"
      , long_desc           => "This token is used when re-materializing a BigQuery table, and merges in new columns from the source table ( ie use when extra columns are being added to the source"
  }
  , COMPLEX_KEYS_AND_MD5SUM_HASH => {
        short_desc          => "MD5SUM data validation token"
      , long_desc           => "This token returns a list of keys, followed by an expression that concatinates all non-key fields, and wraps the results in an MD5SUM function"
  }
  , COMPLEX_CONCAT => {
        short_desc          => "MD5SUM *debugging* only - concat all columns together"
      , long_desc           => "This token is useful for selecting all the columns that COMPLEX_KEYS_AND_MD5SUM_HASH would generate a hash from, so you can compare"
                             . " the resulting string from different databases"
  }
  , COMPLEX_PRIMARY_KEYS_FROM_SOURCE => {
        short_desc          => "Fetch primary keys from source table"
      , long_desc           => "This token queries the source database to fetch a list of keys from the source table."
  }
  , COMPLEX_DELTA_FILTER_CLAUSE => {
        short_desc          => "Generates a filter useful for doing delta loads"
      , long_desc           => "This token requires #P_DELTA_COLUMN# to be set to the delta column, and #Q_MAX_DELTA_VALUE#"
                             . " to be populated with the max value of the delta column in the target table."
  }
  , COMPLEX_COLUMNS_FROM_SOURCE_ENCRYPTED => {
        short_desc          => "Select from source, and encrypt specified columns"
      , long_desc           => "This token generates a select list from the source table, and encrypts columns listed in #P_ENCRYPT_COLUMNS#."
                             . "\n\nEncryption is implemented in the source database class's\nencrypt_expression() method."
  }
  , COMPLEX_UPDATE_STATE_SQL => {
        short_desc          => "Update the USER_DELTA_LOGIC table with the current job's state"
      , long_desc           => "This token is used by Smart Frameworks database migration jobs to update the table status after a delta migration."
  }
  , COMPLEX_CREATE_TMP_FILE => {
        short_desc          => "Create a temporary file and return its path"
      , long_desc           => "This token generates a filename that's guaranteed to be globally unique and safe from race conditions. Remember to delete the file when you're done with it."
  }
};

1;
